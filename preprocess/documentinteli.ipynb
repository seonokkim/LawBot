{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-documentintelligence==1.0.0b4\n",
    "%pip install azure-ai-formrecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code sample shows Prebuilt Layout operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Document Intelligence (formerly Form Recognizer) SDKs\n",
    "https://learn.microsoft.com/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?pivots=programming-language-python\n",
    "\"\"\"\n",
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "endpoint = \"\"\n",
    "key = \"\"\n",
    "\n",
    "formUrl = \"\"\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "    \n",
    "poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-layout\", formUrl)\n",
    "result = poller.result()\n",
    "\n",
    "\n",
    "response_content=result.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def parse_immigration_law(text):\n",
    "    \"\"\"\n",
    "    Function to parse immigration law text into structured JSON format.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The content of the immigration law document.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a parsed article with its metadata.\n",
    "    \"\"\"\n",
    "    # Define regex patterns for chapters, sections, and articles\n",
    "    chapter_pattern = re.compile(r'^제(\\d+)장\\s+(.+)', re.MULTILINE)\n",
    "    section_pattern = re.compile(r'^제(\\d+)절\\s+(.+)', re.MULTILINE)\n",
    "    article_pattern = re.compile(r'^제(\\d+)조(?:의(\\d+))?\\s*\\(([^)]+)\\)', re.MULTILINE)\n",
    "    \n",
    "    # Find all chapters, sections, and articles in the text\n",
    "    chapters = list(chapter_pattern.finditer(text))\n",
    "    sections = list(section_pattern.finditer(text))\n",
    "    articles = list(article_pattern.finditer(text))\n",
    "    \n",
    "    chapter_idx = 0\n",
    "    section_idx = 0\n",
    "    \n",
    "    result = []  # List to store parsed data\n",
    "    \n",
    "    current_chapter_num = None\n",
    "    current_chapter_title = \"\"\n",
    "    current_section_title = \"\"\n",
    "    \n",
    "    # Capture article positions for determining content boundaries\n",
    "    article_starts = [m.start() for m in articles]\n",
    "    article_starts.append(len(text))\n",
    "    \n",
    "    for i, article in enumerate(articles):\n",
    "        article_num = article.group(1)\n",
    "        sub_num = article.group(2) if article.group(2) else \"\"\n",
    "        article_title = article.group(3).strip()\n",
    "        \n",
    "        article_start = article.start()\n",
    "        article_end = article.end()\n",
    "        next_article_start = article_starts[i+1]\n",
    "        \n",
    "        # Extract content between articles\n",
    "        content = text[article_end:next_article_start].strip()\n",
    "        \n",
    "        # Update chapter information if applicable\n",
    "        while chapter_idx < len(chapters) and chapters[chapter_idx].start() < article_start:\n",
    "            current_chapter_num = chapters[chapter_idx].group(1)\n",
    "            current_chapter_title = chapters[chapter_idx].group(2).strip()\n",
    "            current_section_title = \"\"  # Reset section title for new chapter\n",
    "            chapter_idx += 1\n",
    "        \n",
    "        # Update section information if applicable\n",
    "        while section_idx < len(sections) and sections[section_idx].start() < article_start:\n",
    "            section_number = sections[section_idx].group(1)\n",
    "            section_text = sections[section_idx].group(2).strip()\n",
    "            current_section_title = f\"제{section_number}절 {section_text}\"  # Combine section number and title\n",
    "            section_idx += 1\n",
    "        \n",
    "        # Construct chapter ID\n",
    "        if sub_num:\n",
    "            chapter_id = f\"{current_chapter_num}-{article_num}-{sub_num}\"\n",
    "        else:\n",
    "            chapter_id = f\"{current_chapter_num}-{article_num}\"\n",
    "        \n",
    "        # Construct chapter title\n",
    "        chapter_title = f\"{current_chapter_title} - {current_section_title}\" if current_section_title else current_chapter_title\n",
    "        \n",
    "        # Build JSON object for the article\n",
    "        json_obj = {\n",
    "            \"chapter_id\": chapter_id,\n",
    "            \"chapter_title\": chapter_title,\n",
    "            \"title\": article_title,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        result.append(json_obj)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Adjust the input text to ensure proper parsing of chapters and sections\n",
    "response_content = re.sub(r'(제\\d+장[^\\n]+?)\\s+(제\\d+절)', r'\\1\\n\\2', response_content)  # Add newline between chapters and sections\n",
    "\n",
    "# Parse the immigration law content\n",
    "parsed_law = parse_immigration_law(response_content)\n",
    "\n",
    "# Save the parsed data as a JSON file\n",
    "output_file = r\"immigration.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json_output = json.dump(parsed_law, f, ensure_ascii=False, indent=2)  # Save JSON with proper formatting\n",
    "\n",
    "print(f\"Parsed data saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
